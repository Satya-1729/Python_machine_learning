{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDUWiFTDQlnw+lrPTPRb5h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satya-1729/Python_machine_learning/blob/main/evaluating_the_models_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluating the differnt models\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "\n",
        "# reducing the display precison on numpy arrays i.e., decimals value upto 2 decimal places\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# supressing_the_warnings\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "tf.autograph.set_verbosity(0)\n",
        "\n",
        "# now loading the dataset\n",
        "df = pd.read_csv('/content/House Price Prediction Dataset.csv')\n",
        "df.head()\n",
        "\n",
        "x = df.iloc[:,1]\n",
        "print(x[:5],\"\\n\")\n",
        "y = df.iloc[:,-1]\n",
        "\n",
        "print(x.shape,\"\\n\", y.shape)\n",
        "\n",
        "# converting into 2d arrays since data will be in 1d array\n",
        "\n",
        "x = np.expand_dims(x, axis=1)\n",
        "y = np.expand_dims(y, axis=1)\n",
        "\n",
        "print(x.shape,\"\\n\", y.shape,\"\\n\\n\")\n",
        "\n",
        "# now we will split our dataset into training and testing and cross-validation sets\n",
        "\n",
        "x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.4, random_state=42)\n",
        "\n",
        "#now we will split remaing dataset into test and cross-validation set\n",
        "\n",
        "x_test, x_cv, y_test, y_cv = train_test_split(x_, y_, test_size=0.5, random_state=42)\n",
        "\n",
        "print(x_train.shape,\"\\n\", y_train.shape,\"\\n\")\n",
        "print(x_test.shape,\"\\n\", y_test.shape,\"\\n\")\n",
        "print(x_cv.shape,\"\\n\", y_cv.shape)\n",
        "\n",
        "del x_, y_ # deleteing\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train) #it will normalize the dataset\n",
        "x_test_scaled = scaler.transform(x_test) #it will also normalize the dataset using the s_train mean and std deviation\n",
        "x_cv_scaled = scaler.transform(x_cv)#it will also normalize the dataset using the s_train mean and std deviation\n",
        "\n",
        "# we will preview the x_train data\n",
        "print(x_train_scaled[:5],\"\\n\")\n",
        "# now using the model linear regression\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "# now fitting the x_train and y_train\n",
        "model.fit(x_train_scaled, y_train)\n",
        "\n",
        "#now we will evaluate the model using the builtin mean square error function\n",
        "\n",
        "mse = mean_squared_error(y_train, model.predict(x_train_scaled))/2\n",
        "print(f\"Mean Squared Error:, {mse:.2f}\")\n",
        "\n",
        "yhat = model.predict(x_train_scaled)\n",
        "cost = 0\n",
        "# for loop\n",
        "for i in range(len(yhat)):\n",
        "  f_wb_i = (yhat[i] - y_train[i])**2\n",
        "  cost = cost + f_wb_i\n",
        "\n",
        "mse_1 = cost/(2 * len(yhat))\n",
        "print(f\"Total Error: {mse_1.squeeze() : .2f}\")\n",
        "\n",
        "# now we will calculate for cross validation set\n",
        "\n",
        "mse = mean_squared_error(y_cv, model.predict(x_cv_scaled))/2\n",
        "print(f\"Mean Squared Error:, {mse:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woz8T5TDtBKR",
        "outputId": "7fd4ea89-4d30-4c9d-adbf-30392a3be257"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    1360\n",
            "1    4272\n",
            "2    3592\n",
            "3     966\n",
            "4    4926\n",
            "Name: Area, dtype: int64 \n",
            "\n",
            "(2000,) \n",
            " (2000,)\n",
            "(2000, 1) \n",
            " (2000, 1) \n",
            "\n",
            "\n",
            "(1200, 1) \n",
            " (1200, 1) \n",
            "\n",
            "(400, 1) \n",
            " (400, 1) \n",
            "\n",
            "(400, 1) \n",
            " (400, 1)\n",
            "[[ 1.01]\n",
            " [-1.38]\n",
            " [ 1.46]\n",
            " [-0.75]\n",
            " [ 1.7 ]] \n",
            "\n",
            "Mean Squared Error:, 37726001465.90\n",
            "Total Error:  37726001465.90\n",
            "Mean Squared Error:, 39915237841.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#adding the polynomial features\n",
        "\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "x_train_poly = poly.fit_transform(x_train_scaled)\n",
        "x_test_poly = poly.transform(x_test_scaled)\n",
        "x_cv_poly = poly.transform(x_cv_scaled)\n",
        "\n",
        "print(x_train_poly.shape,\"\\n\")\n",
        "print(x_test_poly.shape,\"\\n\")\n",
        "print(x_cv_poly.shape,\"\\n\")\n",
        "\n",
        "print(x_train_poly[0:5],\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# now we will normalize the x train poly and x_cv poly...\n",
        "scaler_poly = StandardScaler()\n",
        "x_train_poly_scaled = scaler_poly.fit_transform(x_train_poly)\n",
        "x_test_poly_scaled = scaler_poly.transform(x_test_poly)\n",
        "x_cv_poly_scaled = scaler_poly.transform(x_cv_poly)\n",
        "\n",
        "poly_model = LinearRegression()\n",
        "poly_model.fit(x_train_poly_scaled, y_train)\n",
        "\n",
        "yhat_poly = poly_model.predict(x_train_poly_scaled)\n",
        "mse = mean_squared_error(y_train, yhat_poly)/2\n",
        "print(f\"Mean Squared Error for training dataset in poly :, {mse:.2f}\")\n",
        "\n",
        "# checking for cross validation dataset before running for test dataset\n",
        "yhat_poly_cv = poly_model.predict(x_cv_poly_scaled)\n",
        "mse = mean_squared_error(y_cv, yhat_poly_cv)/2\n",
        "print(f\"Mean Squared Error for cross validation poly :, {mse:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yc28tCGIFp6",
        "outputId": "0f1b1435-6ace-418b-a054-20563a2dd17b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1200, 2) \n",
            "\n",
            "(400, 2) \n",
            "\n",
            "(400, 2) \n",
            "\n",
            "[[ 1.01  1.01]\n",
            " [-1.38  1.91]\n",
            " [ 1.46  2.14]\n",
            " [-0.75  0.57]\n",
            " [ 1.7   2.87]] \n",
            "\n",
            "Mean Squared Error for training dataset in poly :, 37723299158.37\n",
            "Mean Squared Error for cross validation poly :, 39939688677.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we will evaluate our models woth different polynomial degrees on the same set of the data\n",
        "\n",
        "error_trains = []\n",
        "error_cvs = []\n",
        "poly_degree = []\n",
        "\n",
        "scalers = []\n",
        "models = []\n",
        "\n",
        "for i in range(1,11):\n",
        "  poly = PolynomialFeatures(degree=i, include_bias=False)\n",
        "  x_train_poly_mapped = poly.fit_transform(x_train_scaled)\n",
        "  #instead of using the indexing adding we have to append it cause\n",
        "  #we are overwritting in the list that is not possible for some reasons\n",
        "  poly_degree.append(poly)\n",
        "\n",
        "\n",
        "#scalers used in the code appending to the list\n",
        "  scaler_poly = StandardScaler()\n",
        "  x_train_poly_scaled= scaler_poly.fit_transform(x_train_poly_mapped)\n",
        "  scalers.append(scaler_poly)\n",
        "\n",
        "\n",
        "\n",
        "#appending the models used in this dataset\n",
        "  model = LinearRegression()\n",
        "  model.fit(x_train_poly_scaled, y_train)\n",
        "  models.append(model)\n",
        "\n",
        "\n",
        "#calculating the errors for different degrees and appending to the list error_trains\n",
        "  yhat_poly = model.predict(x_train_poly_scaled)\n",
        "  error_train = mean_squared_error(y_train, yhat_poly)/2\n",
        "  error_trains.append(error_train)\n",
        "\n",
        "#transforming the x_cv dataset into same degree as the degree of x train\n",
        "  x_cv_poly_mapped = poly.transform(x_cv_scaled)\n",
        "  x_cv_poly_scaled = scaler_poly.transform(x_cv_poly_mapped)\n",
        "\n",
        "# checking the error for cross validation dataset\n",
        "  yhat_poly_cv = model.predict(x_cv_poly_scaled)\n",
        "  error_cv = mean_squared_error(y_cv, yhat_poly_cv)/2\n",
        "  error_cvs.append(error_cv)\n",
        "\n",
        "print(poly_degree,\"\\n\")\n",
        "print(scalers,\"\\n\")\n",
        "print(models,\"\\n\")\n",
        "print(error_trains,\"\\n\")\n",
        "print(error_cvs,\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxAPZHF5N8HR",
        "outputId": "98da0137-a3a9-47be-9ca5-c0b143a756cc"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PolynomialFeatures(degree=1, include_bias=False), PolynomialFeatures(include_bias=False), PolynomialFeatures(degree=3, include_bias=False), PolynomialFeatures(degree=4, include_bias=False), PolynomialFeatures(degree=5, include_bias=False), PolynomialFeatures(degree=6, include_bias=False), PolynomialFeatures(degree=7, include_bias=False), PolynomialFeatures(degree=8, include_bias=False), PolynomialFeatures(degree=9, include_bias=False), PolynomialFeatures(degree=10, include_bias=False)] \n",
            "\n",
            "[StandardScaler(), StandardScaler(), StandardScaler(), StandardScaler(), StandardScaler(), StandardScaler(), StandardScaler(), StandardScaler(), StandardScaler(), StandardScaler()] \n",
            "\n",
            "[LinearRegression(), LinearRegression(), LinearRegression(), LinearRegression(), LinearRegression(), LinearRegression(), LinearRegression(), LinearRegression(), LinearRegression(), LinearRegression()] \n",
            "\n",
            "[37726001465.9021, 37723299158.37444, 37658969815.62473, 37621769439.26818, 37617583477.02773, 37574024046.15376, 37566689759.965324, 37564186210.92598, 37521684857.17097, 37518032374.39536] \n",
            "\n",
            "[39915237841.61203, 39939688677.688576, 39736556923.291916, 39778460352.76487, 39804814274.65897, 39710742872.94479, 39732510391.28169, 39735382417.62267, 39877591711.60986, 39823478807.30667] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reducing the display precison on numpy arrays i.e., decimals value upto 2 decimal places\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# choosing the best model\n",
        "# now we will find the index of lowest error_trains i.e., lowest training error in a list\n",
        "\n",
        "#using argmin function to calculate the index of minimum error value above all the errror values\n",
        "degree_use = np.argmin(error_trains)+1\n",
        "print(degree_use)\n",
        "\n",
        "degree_use_cv = np.argmin(error_cvs)+1 #here we add because index starts at 0\n",
        "print(degree_use_cv)\n",
        "\n",
        "# now after seeing the results on the cv dataset we will use the model with the degree 6\n",
        "#to transform our testing dataset\n",
        "x_test_poly_mapped = poly_degree[degree_use_cv-1].transform(x_test_scaled)\n",
        "\n",
        "# now we will normalize the datset using same std deviation and mean\n",
        "x_test_poly_scaled = scalers[degree_use_cv-1].transform(x_test_poly_mapped)\n",
        "\n",
        "# in this below line we calculate the test predicted value in order to calculate the error for test data\n",
        "\n",
        "yhat = models[degree_use_cv-1].predict(x_test_poly_scaled)\n",
        "\n",
        "# error calculation\n",
        "error_test_f = mean_squared_error(yhat, y_test)/2\n",
        "print(f\"{error_test_f:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qxkz_iXPTL97",
        "outputId": "d01eb2d0-c22c-4894-cda3-66485792abe8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "6\n",
            "38258274364.81\n"
          ]
        }
      ]
    }
  ]
}